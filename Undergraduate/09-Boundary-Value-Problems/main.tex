\documentclass[12pt,a4paper]{article}

% ============================================================
%  PACKAGES
% ============================================================
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{enumitem}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{tcolorbox}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{fancyhdr}
\usepackage{titlesec}
\usepackage{array}

% ============================================================
%  COLORS & STYLE
% ============================================================
\definecolor{primaryblue}{RGB}{0, 71, 171}
\definecolor{accentred}{RGB}{180, 40, 40}
\definecolor{lightgray}{RGB}{245, 245, 245}
\definecolor{darkgreen}{RGB}{0, 120, 60}

\hypersetup{
    colorlinks=true,
    linkcolor=primaryblue,
    urlcolor=primaryblue,
}

% Section styling
\titleformat{\section}
  {\Large\bfseries\color{primaryblue}}{\thesection}{1em}{}
\titleformat{\subsection}
  {\large\bfseries\color{primaryblue!80!black}}{\thesubsection}{1em}{}
\titleformat{\subsubsection}
  {\normalsize\bfseries\color{primaryblue!60!black}}{\thesubsubsection}{1em}{}

% Header/Footer
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{\small\textcolor{primaryblue}{Numerical Analysis Bootcamp}}
\fancyhead[R]{\small\textcolor{primaryblue}{Unit 9: Boundary Value Problems}}
\fancyfoot[C]{\thepage}
\renewcommand{\headrulewidth}{0.4pt}

% ============================================================
%  CUSTOM ENVIRONMENTS
% ============================================================
\tcbuselibrary{skins, breakable}

% Key Concept box
\newtcolorbox{keyconcept}[1][]{
  colback=primaryblue!5,
  colframe=primaryblue,
  fonttitle=\bfseries,
  title={Key Concept},
  breakable,
  #1
}

% Example box
\newtcolorbox{example}[1][]{
  colback=darkgreen!5,
  colframe=darkgreen,
  fonttitle=\bfseries,
  title={Example},
  breakable,
  #1
}

% Warning box
\newtcolorbox{warning}[1][]{
  colback=accentred!5,
  colframe=accentred,
  fonttitle=\bfseries,
  title={Warning},
  breakable,
  #1
}

% Takeaway box
\newtcolorbox{takeaway}[1][]{
  colback=yellow!8,
  colframe=orange!80!black,
  fonttitle=\bfseries,
  title={Takeaway},
  breakable,
  #1
}

% ============================================================
%  DOCUMENT
% ============================================================
\begin{document}

% Title Page
\begin{titlepage}
\centering
\vspace*{2cm}
{\Huge\bfseries\color{primaryblue} Numerical Analysis Bootcamp \\[0.5cm]}
{\LARGE Undergraduate Level \\[1.5cm]}
{\rule{\textwidth}{1.5pt} \\[0.5cm]}
{\huge\bfseries Unit 9 \\[0.3cm]}
{\LARGE Boundary Value Problems \\[0.5cm]}
{\rule{\textwidth}{1.5pt} \\[2cm]}
{\large
When conditions are prescribed at two different points,\\
marching forward no longer works. The shooting method\\
and finite differences tackle the problem from opposite directions.
\\[3cm]}
{\large\today}
\end{titlepage}

\tableofcontents
\newpage

% ============================================================
%  SECTION 9.1
% ============================================================
\section{Initial Value Problems vs.\ Boundary Value Problems}

\subsection{A New Kind of Problem}

In Unit 8, we solved \textbf{initial value problems} (IVPs): given $y' = f(t,y)$ and a starting value $y(t_0) = y_0$, we marched forward in time to trace the solution. All the information we needed was concentrated at a single point.

Now we encounter a fundamentally different situation:

\begin{keyconcept}[title={Boundary Value Problem (BVP)}]
A \textbf{boundary value problem} specifies conditions at \textbf{two or more} distinct points. The standard two-point BVP has the form:
\[
y'' = f(x, y, y'), \qquad y(a) = \alpha, \quad y(b) = \beta.
\]
The conditions $y(a) = \alpha$ and $y(b) = \beta$ are called \textbf{boundary conditions}.
\end{keyconcept}

The crucial difference: in an IVP, we know everything at one end and march to the other. In a BVP, we know something at \emph{both} ends but nothing in between. We cannot simply march forward---the solution must satisfy constraints at both boundaries \emph{simultaneously}.

\subsection{Physical Motivation}

BVPs arise naturally in steady-state and equilibrium problems:

\begin{itemize}[leftmargin=2em]
  \item \textbf{Heat conduction in a rod.} A metal rod has its left end held at $100^\circ$C and its right end at $200^\circ$C. What is the temperature distribution along the rod at equilibrium? This gives $T'' = f(x)$ with $T(0) = 100$, $T(L) = 200$.

  \item \textbf{Beam deflection.} A beam is fixed at both ends and loaded. The deflection $y(x)$ satisfies a fourth-order ODE with conditions at both supports.

  \item \textbf{Electrostatics.} The voltage in a region satisfies Laplace's equation with prescribed values on the boundary.
\end{itemize}

A useful rule of thumb: \textbf{time-dependent} problems tend to give IVPs, while \textbf{steady-state} or \textbf{equilibrium} problems tend to give BVPs.

\subsection{Why BVPs Are Harder}

\begin{warning}
BVPs are fundamentally harder than IVPs for several reasons:
\begin{itemize}[leftmargin=2em]
  \item \textbf{No marching.} We cannot simply step forward from $x = a$, because we do not know $y'(a)$---only $y(a)$ and $y(b)$.
  \item \textbf{Global coupling.} The value of $y$ at any interior point depends on the boundary conditions at \emph{both} ends. The problem is inherently global.
  \item \textbf{Existence is not guaranteed.} Unlike IVPs (where the Picard--Lindel\"{o}f theorem guarantees a unique local solution under mild conditions), a BVP may have no solution, exactly one solution, or infinitely many solutions.
\end{itemize}
\end{warning}

We will study two fundamentally different strategies:
\begin{enumerate}[leftmargin=2em]
  \item \textbf{The shooting method} --- convert the BVP into an IVP and reuse our Unit 8 solvers.
  \item \textbf{Finite differences} --- discretize the entire domain and solve a system of algebraic equations.
\end{enumerate}

\begin{takeaway}
A boundary value problem prescribes conditions at two (or more) distinct points, unlike an IVP where all conditions are at one point. BVPs arise naturally in steady-state and equilibrium problems. They cannot be solved by marching forward and require either the shooting method or finite difference discretization.
\end{takeaway}

\newpage
% ============================================================
%  SECTION 9.2
% ============================================================
\section{The Shooting Method}

The shooting method is a clever strategy that converts a BVP into a sequence of IVPs---allowing us to reuse the solvers from Unit 8.

\subsection{The Key Idea}

Consider the BVP: $y'' = f(x, y, y')$, $y(a) = \alpha$, $y(b) = \beta$.

We know $y(a) = \alpha$, but we do \textbf{not} know $y'(a)$. If we \emph{did} know $y'(a)$, the problem would be an IVP and we could march forward with Euler or RK4.

\begin{keyconcept}[title={The Shooting Method}]
\begin{enumerate}
  \item \textbf{Guess} the missing initial slope: $y'(a) = s$.
  \item \textbf{Solve the IVP}: $y'' = f(x,y,y')$, $y(a) = \alpha$, $y'(a) = s$, using any IVP solver (e.g., RK4) from $x = a$ to $x = b$.
  \item \textbf{Check} the result at $x = b$: compute $\varphi(s) = y(b; s) - \beta$.
  \item If $\varphi(s) = 0$, we have found the correct slope---done!
  \item If $\varphi(s) \neq 0$, \textbf{adjust} $s$ and repeat (using root-finding methods from Unit 2).
\end{enumerate}
The BVP has been reduced to a \textbf{root-finding problem} in the single unknown $s$.
\end{keyconcept}

The name ``shooting'' comes from an analogy: imagine aiming a cannon at a target. You choose an angle (the initial slope $s$), fire (solve the IVP), and see where the cannonball lands. If you miss the target, you adjust the angle and fire again.

\subsection{A Worked Example}

\begin{example}[title={Shooting Method}]
Solve the BVP: $y'' - y = 0$, $y(0) = 0$, $y(1) = \sinh(1) \approx 1.1752$.

The exact solution is $y(x) = \sinh(x)$, with $y'(0) = \cosh(0) = 1$.

\medskip
\textbf{Step 1: Set up the IVP.} For a given guess $s$, solve:
\[
y'' = y, \quad y(0) = 0, \quad y'(0) = s.
\]
The IVP has the analytical solution $y(x; s) = s \cdot \sinh(x)$, so $y(1; s) = s \cdot \sinh(1) \approx 1.1752\,s$.

Define the residual: $\varphi(s) = y(1; s) - 1.1752 = 1.1752\,s - 1.1752 = 1.1752\,(s - 1)$.

\medskip
\textbf{Step 2: Guess and check.}

\medskip
\begin{center}
\begin{tabular}{@{} crrl @{}}
\toprule
Guess & $y(1; s)$ & $\varphi(s)$ & Verdict \\
\midrule
$s_1 = 0.5$ & $0.588$ & $-0.588$ & Too low \\
$s_2 = 2.0$ & $2.350$ & $+1.175$ & Too high \\
\bottomrule
\end{tabular}
\end{center}
\medskip

Since $\varphi(0.5) < 0$ and $\varphi(2.0) > 0$, the root lies between $0.5$ and $2.0$.

\medskip
\textbf{Step 3: Apply the secant method.}
\[
s_3 = s_1 - \varphi(s_1)\,\frac{s_2 - s_1}{\varphi(s_2) - \varphi(s_1)} = 0.5 - (-0.588)\,\frac{1.5}{1.175 - (-0.588)} = 0.5 + \frac{0.882}{1.763} = 0.5 + 0.500 = 1.000.
\]

\textbf{Verify:} $y(1; 1) = 1 \cdot 1.1752 = 1.1752 = \sinh(1)$. \checkmark

The secant method found the exact slope $s = 1$ in a single step---because the problem is linear.
\end{example}

\subsection{Linear vs.\ Nonlinear BVPs}

For \textbf{linear} BVPs, $\varphi(s)$ is a linear function of $s$ (by the principle of superposition). This means:
\begin{itemize}[leftmargin=2em]
  \item Two guesses are sufficient---linear interpolation gives the exact $s$.
  \item The shooting method always converges in one interpolation step.
\end{itemize}

For \textbf{nonlinear} BVPs, $\varphi(s)$ is nonlinear, and we must use iterative root-finding (bisection, secant, or Newton's method). Convergence is no longer guaranteed, and the result may depend on the initial guess.

\subsection{Strengths and Limitations}

\begin{center}
\begin{tabular}{@{} p{0.44\textwidth} p{0.44\textwidth} @{}}
\toprule
\textbf{Strengths} & \textbf{Limitations} \\
\midrule
Reuses existing IVP solvers (RK4). & Sensitive to the initial guess $s$ for nonlinear problems. \\[4pt]
Conceptually simple. & Can be numerically unstable for some BVPs (exponentially growing solutions). \\[4pt]
High accuracy via high-order IVP methods. & May miss solutions if the problem has multiple solutions. \\
\bottomrule
\end{tabular}
\end{center}

\begin{takeaway}
The shooting method converts a BVP into a root-finding problem by guessing the missing initial slope $s$, solving the IVP, and checking the result at the other boundary. It reduces the BVP to $\varphi(s) = 0$, which can be solved with bisection or the secant method. For linear BVPs, one interpolation step suffices. For nonlinear BVPs, iterative root-finding is needed.
\end{takeaway}

\newpage
% ============================================================
%  SECTION 9.3
% ============================================================
\section{Finite Difference Method for BVPs}

The finite difference method takes a completely different approach: instead of converting the BVP to an IVP, we discretize the entire domain at once and solve a system of algebraic equations.

\subsection{Discretizing the Domain}

Divide $[a, b]$ into $N$ equal subintervals of width $h = (b - a)/N$, creating $N + 1$ grid points:
\[
x_i = a + ih, \qquad i = 0, 1, 2, \ldots, N.
\]

We seek approximate values $y_0, y_1, \ldots, y_N$ at these grid points. The boundary conditions fix $y_0 = \alpha$ and $y_N = \beta$, leaving $N - 1$ \textbf{interior unknowns}: $y_1, y_2, \ldots, y_{N-1}$.

\subsection{Replacing Derivatives with Finite Differences}

From Units 6 and 7, we know how to approximate derivatives using neighboring values:

\begin{keyconcept}[title={Finite Difference Approximations}]
\begin{align*}
y''(x_i) &\approx \frac{y_{i-1} - 2y_i + y_{i+1}}{h^2} \quad \text{(second derivative, $O(h^2)$)}, \\[6pt]
y'(x_i) &\approx \frac{y_{i+1} - y_{i-1}}{2h} \quad \text{(first derivative, $O(h^2)$)}.
\end{align*}
\end{keyconcept}

\subsection{From ODE to Algebraic Equations}

Consider the BVP $y'' = g(x)$, $y(a) = \alpha$, $y(b) = \beta$ (the simplest case: no $y$ or $y'$ on the right-hand side).

Substituting the central difference approximation at each interior node:

\begin{keyconcept}[title={Finite Difference Equations}]
At each interior point $x_i$ ($i = 1, 2, \ldots, N-1$):
\[
\frac{y_{i-1} - 2y_i + y_{i+1}}{h^2} = g(x_i).
\]
Rearranging:
\[
y_{i-1} - 2y_i + y_{i+1} = h^2\, g(x_i).
\]
This gives $N - 1$ equations in $N - 1$ unknowns.
\end{keyconcept}

Each equation connects three consecutive unknowns: $y_{i-1}$, $y_i$, $y_{i+1}$. At the boundaries ($i = 1$ and $i = N-1$), the known values $y_0 = \alpha$ and $y_N = \beta$ are moved to the right-hand side.

The key insight: the BVP has been transformed from a \emph{differential} equation into a \emph{system of linear algebraic equations}---exactly the kind of problem we studied in Unit 3.

\begin{takeaway}
The finite difference method replaces the continuous BVP with a discrete system of algebraic equations on a grid. Second derivatives are approximated by the 3-point central difference stencil $\frac{y_{i-1} - 2y_i + y_{i+1}}{h^2}$. The boundary conditions provide the values at the endpoints, leaving $N - 1$ interior unknowns connected by $N - 1$ equations.
\end{takeaway}

\newpage
% ============================================================
%  SECTION 9.4
% ============================================================
\section{Setting Up and Solving the Linear System}

\subsection{The Tridiagonal Structure}

Writing the finite difference equations from Section 3 in matrix form reveals a beautiful structure:

\begin{keyconcept}[title={Tridiagonal System}]
\[
\underbrace{\begin{pmatrix}
-2 & 1 & 0 & \cdots & 0 \\
1 & -2 & 1 & \cdots & 0 \\
0 & 1 & -2 & \cdots & 0 \\
\vdots & & \ddots & \ddots & 1 \\
0 & \cdots & 0 & 1 & -2
\end{pmatrix}}_{\mathbf{A}}
\begin{pmatrix} y_1 \\ y_2 \\ y_3 \\ \vdots \\ y_{N-1} \end{pmatrix}
=
\begin{pmatrix} h^2 g(x_1) - \alpha \\ h^2 g(x_2) \\ h^2 g(x_3) \\ \vdots \\ h^2 g(x_{N-1}) - \beta \end{pmatrix}.
\]
The matrix $\mathbf{A}$ is \textbf{tridiagonal}: nonzero entries appear only on the main diagonal and the two adjacent diagonals.
\end{keyconcept}

Why tridiagonal? Because each finite difference equation involves only three consecutive unknowns. The boundary values $\alpha$ and $\beta$ appear in the first and last entries of the right-hand side, respectively.

Tridiagonal systems can be solved in $O(N)$ operations using the \textbf{Thomas algorithm}---a specialized form of Gaussian elimination (Unit 3). This makes the finite difference method extremely efficient, even for very fine grids.

\subsection{A Full Worked Example}

\begin{example}[title={Finite Differences: Full Worked Problem}]
Solve the BVP:
\[
y'' = 12x^2 - 4, \qquad y(0) = 1, \quad y(1) = 3.
\]

\textbf{Exact solution:} $y = x^4 - 2x^2 + 3x + 1$. (Verify: $y'' = 12x^2 - 4$ \checkmark, $y(0) = 1$ \checkmark, $y(1) = 3$ \checkmark.)

\medskip
\textbf{Discretize} with $N = 4$ ($h = 0.25$). Grid: $x_0 = 0$, $x_1 = 0.25$, $x_2 = 0.50$, $x_3 = 0.75$, $x_4 = 1$. Known: $y_0 = 1$, $y_4 = 3$. Unknowns: $y_1, y_2, y_3$.

\medskip
\textbf{Right-hand side values} $h^2 \cdot g(x_i) = 0.0625 \cdot (12x_i^2 - 4)$:

\medskip
\begin{center}
\begin{tabular}{@{} cccc @{}}
\toprule
$i$ & $x_i$ & $12x_i^2 - 4$ & $h^2 \cdot g(x_i)$ \\
\midrule
1 & 0.25 & $-3.25$ & $-0.2031$ \\
2 & 0.50 & $-1.00$ & $-0.0625$ \\
3 & 0.75 & $+2.75$ & $+0.1719$ \\
\bottomrule
\end{tabular}
\end{center}
\medskip

\textbf{Assemble the system} (incorporating boundary values):
\[
\begin{pmatrix} -2 & 1 & 0 \\ 1 & -2 & 1 \\ 0 & 1 & -2 \end{pmatrix}
\begin{pmatrix} y_1 \\ y_2 \\ y_3 \end{pmatrix}
=
\begin{pmatrix} -0.2031 - 1 \\ -0.0625 \\ 0.1719 - 3 \end{pmatrix}
=
\begin{pmatrix} -1.2031 \\ -0.0625 \\ -2.8281 \end{pmatrix}.
\]

\textbf{Solve} by forward elimination:
\begin{itemize}
  \item From row 1: $y_2 = 2y_1 - 1.2031$.
  \item Substitute into row 2: $-3y_1 + y_3 = -2.4688$, so $y_3 = 3y_1 - 2.4688$.
  \item Substitute both into row 3: $-4y_1 = -6.5625$, giving $y_1 = 1.6406$.
  \item Back-substitute: $y_2 = 2.0781$, $y_3 = 2.4531$.
\end{itemize}

\medskip
\textbf{Compare with exact:}

\medskip
\begin{center}
\begin{tabular}{@{} ccccr @{}}
\toprule
$i$ & $x_i$ & FD solution & Exact & Error \\
\midrule
1 & 0.25 & 1.6406 & 1.6289 & 0.0117 \\
2 & 0.50 & 2.0781 & 2.0625 & 0.0156 \\
3 & 0.75 & 2.4531 & 2.4414 & 0.0117 \\
\bottomrule
\end{tabular}
\end{center}
\medskip

The maximum error is $0.016$---about $0.8\%$. Not bad for only 3 interior unknowns!
\end{example}

\begin{takeaway}
For linear BVPs, the finite difference method produces a tridiagonal system $\mathbf{A}\mathbf{y} = \mathbf{b}$. The tridiagonal structure arises from the 3-point stencil and allows $O(N)$ solution via the Thomas algorithm. Boundary values enter the right-hand side of the first and last equations.
\end{takeaway}

\newpage
% ============================================================
%  SECTION 9.5
% ============================================================
\section{Convergence and Accuracy}

\subsection{Truncation Error}

The central difference approximation for $y''$ satisfies:
\[
\frac{y_{i-1} - 2y_i + y_{i+1}}{h^2} = y''(x_i) + \frac{h^2}{12}\,y^{(4)}(x_i) + O(h^4).
\]

The leading error term is $\frac{h^2}{12}\,y^{(4)}(x_i)$, so the \textbf{local truncation error} is $O(h^2)$. It can be shown that the global error---the maximum difference between the finite difference solution and the true solution---is also $O(h^2)$:

\begin{keyconcept}[title={Finite Difference Convergence}]
For a well-posed linear BVP solved with central differences:
\[
\max_{1 \leq i \leq N-1} |y_i^{\text{FD}} - y(x_i)| = O(h^2).
\]
Halving $h$ (doubling $N$) reduces the maximum error by a factor of approximately $\mathbf{4}$.
\end{keyconcept}

\subsection{Convergence in Action}

Returning to our worked example ($y'' = 12x^2 - 4$, $y(0) = 1$, $y(1) = 3$):

\begin{example}[title={Convergence Table}]
\begin{center}
\begin{tabular}{@{} cccr @{}}
\toprule
$N$ & $h$ & Max error & Ratio \\
\midrule
4 & 0.2500 & $1.56 \times 10^{-2}$ & --- \\
8 & 0.1250 & $3.91 \times 10^{-3}$ & 4.0 \\
16 & 0.0625 & $9.77 \times 10^{-4}$ & 4.0 \\
32 & 0.0313 & $2.44 \times 10^{-4}$ & 4.0 \\
\bottomrule
\end{tabular}
\end{center}

Each doubling of $N$ reduces the error by exactly a factor of 4, confirming $O(h^2)$ convergence. By $N = 32$ (31 interior unknowns), the solution is accurate to nearly 4 decimal places.
\end{example}

The error ratios are exactly 4 in this example because the exact solution is a polynomial of degree 4, making the truncation error a clean function of $h^2$. For more general problems, the ratio approaches 4 as $h \to 0$.

\subsection{Checking Your Solution}

Since we do not usually know the exact solution, how can we verify that the finite difference solution is accurate?

\begin{keyconcept}[title={Practical Verification}]
\begin{enumerate}
  \item \textbf{Grid refinement.} Solve with $N$ and $2N$. If the results agree to $d$ digits, you likely have $d$ correct digits.
  \item \textbf{Richardson extrapolation.} If $y_h$ is the solution with step $h$ and $y_{h/2}$ with step $h/2$, the improved estimate $\frac{4\,y_{h/2} - y_h}{3}$ has error $O(h^4)$---two orders better!
  \item \textbf{Residual check.} Substitute the computed $y_i$ back into the original ODE. The residuals $y_{i-1} - 2y_i + y_{i+1} - h^2 g(x_i)$ should be small.
\end{enumerate}
\end{keyconcept}

\begin{takeaway}
The finite difference method with central differences is $O(h^2)$ accurate: doubling $N$ reduces the error by a factor of 4. Grid refinement and Richardson extrapolation provide practical tools for checking accuracy without knowing the exact solution. The convergence rate matches the truncation error of the central difference stencil.
\end{takeaway}

\newpage
% ============================================================
%  SECTION 9.6
% ============================================================
\section{Nonlinear BVPs and Practical Considerations}

\subsection{What Changes When the ODE Is Nonlinear?}

All the examples so far had the form $y'' = g(x)$---the right-hand side depended only on $x$, not on $y$. When $f$ depends on $y$ or $y'$, the finite difference equations become \textbf{nonlinear}.

\begin{example}[title={A Nonlinear BVP}]
Consider the BVP:
\[
y'' = e^y, \qquad y(0) = 0, \quad y(1) = 0.
\]

The finite difference equation at node $i$ is:
\[
\frac{y_{i-1} - 2y_i + y_{i+1}}{h^2} = e^{y_i}.
\]

This is a \textbf{nonlinear} equation in the unknowns $y_i$, because of the $e^{y_i}$ term. We cannot write the system as $\mathbf{A}\mathbf{y} = \mathbf{b}$ and solve it directly.
\end{example}

\subsection{Solving Nonlinear Systems: Newton's Method}

The standard approach for nonlinear BVPs is \textbf{Newton's method for systems}, a generalization of the scalar Newton's method from Unit 2.

\begin{keyconcept}[title={Newton's Method for Nonlinear BVPs}]
Define the vector of residuals $\mathbf{F}(\mathbf{y})$, where each component is one finite difference equation. Newton's method iterates:
\[
\mathbf{y}^{(k+1)} = \mathbf{y}^{(k)} - \mathbf{J}(\mathbf{y}^{(k)})^{-1}\,\mathbf{F}(\mathbf{y}^{(k)}),
\]
where $\mathbf{J}$ is the \textbf{Jacobian matrix} (the matrix of partial derivatives of $\mathbf{F}$).
\begin{enumerate}
  \item Start with an initial guess $\mathbf{y}^{(0)}$ (e.g., a straight line between the boundary values).
  \item At each iteration, solve a \emph{linear} tridiagonal system for the correction.
  \item Repeat until $\|\mathbf{F}(\mathbf{y}^{(k)})\|$ is below a tolerance.
\end{enumerate}
\end{keyconcept}

The Jacobian inherits the tridiagonal structure from the finite difference stencil, so each Newton step is cheap ($O(N)$ work). Convergence is typically quadratic---just as in the scalar case---provided the initial guess is close enough.

\subsection{Challenges with Nonlinear BVPs}

\begin{warning}
Nonlinear BVPs introduce several complications that linear BVPs do not have:
\begin{itemize}[leftmargin=2em]
  \item \textbf{Multiple solutions.} The BVP $y'' + e^y = 0$, $y(0) = y(1) = 0$ has \emph{two} distinct solutions. Different initial guesses may converge to different solutions.
  \item \textbf{No solution.} Some nonlinear BVPs have no solution at all.
  \item \textbf{Sensitivity to the initial guess.} Newton's method may diverge if the initial guess is poor.
\end{itemize}
\end{warning}

\subsection{Shooting Method vs.\ Finite Differences}

Both methods extend naturally to nonlinear BVPs. Here is a practical comparison:

\begin{center}
\begin{tabular}{@{} p{0.44\textwidth} p{0.44\textwidth} @{}}
\toprule
\textbf{Shooting method} & \textbf{Finite differences} \\
\midrule
Converts BVP to IVP + root-finding. Leverages existing IVP solvers. & Discretizes the entire domain at once. Solves a system of algebraic equations. \\[4pt]
High accuracy from high-order IVP solvers (RK4). & Accuracy determined by the grid spacing ($O(h^2)$ for central differences). \\[4pt]
Can be unstable for problems with exponentially growing solutions. & Generally more robust and stable. \\[4pt]
Easy to implement if you already have an IVP solver. & Matrix structure (tridiagonal) allows very efficient solution. \\[4pt]
Extends to nonlinear BVPs via root-finding on $\varphi(s) = 0$. & Extends to nonlinear BVPs via Newton's method on $\mathbf{F}(\mathbf{y}) = \mathbf{0}$. \\
\bottomrule
\end{tabular}
\end{center}

In practice, finite differences are the more common choice for BVPs, especially in higher dimensions (where shooting becomes impractical). The shooting method remains useful for one-dimensional problems where a good IVP solver is already available.

\begin{takeaway}
Nonlinear BVPs require iterative solution methods, typically Newton's method applied to the system of finite difference equations. The Jacobian inherits the tridiagonal structure, keeping each iteration efficient. Nonlinear problems may have multiple solutions or none, and the initial guess matters. Finite differences are generally preferred over shooting for robustness, while shooting leverages high-order IVP solvers for accuracy.
\end{takeaway}

\newpage
% ============================================================
%  SUMMARY
% ============================================================
\section*{Unit 9 Summary}
\addcontentsline{toc}{section}{Unit 9 Summary}

This unit introduced boundary value problems and two fundamental strategies for solving them numerically.

\begin{enumerate}[leftmargin=2em]
  \item \textbf{BVPs vs.\ IVPs.} Boundary value problems specify conditions at two distinct points, unlike IVPs which specify everything at one point. BVPs arise in steady-state and equilibrium problems. They are harder than IVPs because they require global information and cannot be solved by marching forward.

  \item \textbf{The shooting method} converts a BVP into a root-finding problem: guess the missing initial slope $s$, solve the IVP, check the result at the far boundary, and adjust using secant or bisection. For linear BVPs, one interpolation step suffices.

  \item \textbf{Finite differences} discretize the domain into a grid and replace derivatives with central difference formulas. The BVP becomes a system of algebraic equations with $N - 1$ interior unknowns.

  \item \textbf{The linear system} has a tridiagonal matrix structure (from the 3-point stencil), which can be solved in $O(N)$ operations via the Thomas algorithm. Boundary values enter the right-hand side.

  \item \textbf{Convergence} is $O(h^2)$ for central differences: doubling $N$ reduces the error by a factor of 4. Grid refinement and Richardson extrapolation provide practical verification tools.

  \item \textbf{Nonlinear BVPs} produce nonlinear algebraic systems that require iterative methods (Newton's method). The tridiagonal Jacobian keeps each iteration efficient. Multiple solutions or no solution may exist.
\end{enumerate}

\medskip
Comparison of the two approaches:

\begin{center}
\begin{tabular}{@{} lcc @{}}
\toprule
\textbf{Feature} & \textbf{Shooting} & \textbf{Finite Differences} \\
\midrule
Strategy & IVP + root-finding & Discretize entire domain \\
Linear BVP & 1 interpolation step & Tridiagonal solve, $O(N)$ \\
Nonlinear BVP & Iterative root-finding on $s$ & Newton's method on system \\
Accuracy & Depends on IVP solver order & $O(h^2)$ with central differences \\
Robustness & Can be unstable & Generally robust \\
Higher dimensions & Impractical & Extends naturally \\
\bottomrule
\end{tabular}
\end{center}

Key connection: the finite difference method ties together themes from across the bootcamp---finite differences (Unit 6), solving linear systems (Unit 3), and root-finding for nonlinear problems (Unit 2). The shooting method connects IVP solvers (Unit 8) with root-finding (Unit 2).

\end{document}
